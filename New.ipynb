{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "import warnings \n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = df.sample(1200, random_state=786)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'blue':'bluetooth', 'fc':'front_cam_mp', 'sc_h':'screen_ht', 'sc_w':'screen_wt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'pc':'back_cam_mp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['m_dep', 'mobile_wt', 'px_height', 'px_width'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['bluetooth', 'clock_speed', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']\n",
    "continuous_features = ['battery_power', 'front_cam_mp', 'int_memory', 'n_cores', 'back_cam_mp', 'ram']\n",
    "TARGET = ['price_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[continuous_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['battery_power'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bluetooth'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clock_speed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dual_sim'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['front_cam_mp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['four_g'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_memory'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_cores'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['back_cam_mp'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ram'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['screen_ht'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['screen_wt'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['talk_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['three_g'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['touch_screen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['four_g'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wifi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_range'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clock_speed'] = pd.cut(df['clock_speed'], bins=[0, 1, 2, 3], labels = ['low', 'mid', 'high'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_map = {'low':0, 'mid':1, 'high':2}\n",
    "df['clock_speed'] = df['clock_speed'].replace(level_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['screen_ht', 'screen_wt']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['screen_ht'] = df['screen_ht'].astype(float)\n",
    "df['screen_wt'] = df['screen_wt'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[df['screen_wt']==0]['screen_ht'].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = []\n",
    "for d in df.loc[df['screen_wt']==0]['screen_ht']:\n",
    "    arr.append(d)\n",
    "\n",
    "set(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_width = {}\n",
    "for d in set(arr):\n",
    "    total = 0\n",
    "    n = 0\n",
    "    for width in df.loc[df['screen_ht'] == d]['screen_wt']:\n",
    "        if width == 0:\n",
    "            pass\n",
    "        total += width\n",
    "        n += 1\n",
    "        mean = round(total/n, 2)\n",
    "    print(\"Mean width for height\", d, \"=\", mean)\n",
    "    mean_width[d] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in x:\n",
    "    df['screen_wt'] = np.where(((df['screen_wt']==0.0) & (df['screen_ht']==z)), mean_width.get(z), df['screen_wt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['screen_size'] = df['screen_ht']**2 + df['screen_wt']**2\n",
    "df['screen_size'] = np.sqrt(df['screen_size'])\n",
    "df['screen_size'] = df['screen_size']/2.54\n",
    "df['screen_size'] = df['screen_size'].round(2)\n",
    "df.drop(columns=['screen_ht', 'screen_wt'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(df['price_range'])\n",
    "df.drop(columns=TARGET, inplace=True)\n",
    "df = df.join(p)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualisation\n",
    "## 1 variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot\n",
    "fig = plt.figure(figsize = (6, 4))\n",
    "title = fig.suptitle(\"No. of Cores vs.Frequency\", fontsize = 14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel(\"No.of Cores\")\n",
    "ax.set_ylabel(\"Frequency\") \n",
    "w_q = df['n_cores'].value_counts()\n",
    "w_q = (list(w_q.index), list(w_q.values))\n",
    "ax.tick_params(axis='both', which='major', labelsize=8.5)\n",
    "bar = ax.bar(w_q[0], w_q[1], color='steelblue', \n",
    "        edgecolor='black', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['low', 'mid', 'high']\n",
    "df['clock_speed'].value_counts().plot(kind='pie', autopct='%.2f')\n",
    "plt.tight_layout()\n",
    "plt.legend(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['screen_size'].plot(kind='density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(df['price_range'], df['ram'], bins=(4, 16), cmap='Blues')\n",
    "cb = plt.colorbar()\n",
    "cb.set_label('counts in bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Using subplots or facets along with Bar Plots\n",
    "fig = plt.figure(figsize = (10, 4))\n",
    "title = fig.suptitle(\"Dual_sim vs. talk_time\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "# Non Dual Sim\n",
    "ax1 = fig.add_subplot(1,2, 1)\n",
    "ax1.set_title(\"Non Dual Sim\")\n",
    "ax1.set_xlabel(\"Talk-Time\")\n",
    "ax1.set_ylabel(\"Frequency\") \n",
    "rw_q = df[df['dual_sim'] == 0]['talk_time'].value_counts()\n",
    "rw_q = (list(rw_q.index), list(rw_q.values))\n",
    "ax1.set_ylim([0,70])\n",
    "ax1.tick_params(axis='both', which='major', labelsize=8.5)\n",
    "bar1 = ax1.bar(rw_q[0], rw_q[1], color='red', \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Dual Simw\n",
    "ax2 = fig.add_subplot(1,2, 2)\n",
    "ax2.set_title(\"Dual Sim\")\n",
    "ax2.set_xlabel(\"Talk-time\")\n",
    "ax2.set_ylabel(\"Frequency\") \n",
    "ww_q = df[df['dual_sim'] == 1]['talk_time'].value_counts()\n",
    "ww_q = (list(ww_q.index), list(ww_q.values))\n",
    "ax2.set_ylim([0, 70])\n",
    "ax2.tick_params(axis='both', which='major', labelsize=8.5)\n",
    "bar2 = ax2.bar(ww_q[0], ww_q[1], color='white', \n",
    "               edgecolor='black', linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='battery_power', by='price_range')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Scaling attribute values to avoid few outiers\n",
    "cols = ['ram', 'int_memory', 'screen_size', 'battery_power','price_range']\n",
    "pp = sns.pairplot(df[cols], hue='price_range', size=1.8, aspect=1.8, \n",
    "                  palette={0: \"#FF9999\", 1: \"#FFE888\", 2:\"#2A9D8F\", 3:\"#E63946\"},\n",
    "                  plot_kws=dict(edgecolor=\"black\", linewidth=0.5))\n",
    "fig = pp.fig \n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "t = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(y=\"int_memory\", x=\"front_cam_mp\", hue='four_g', kind=\"line\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(df['battery_power'], df['back_cam_mp'], df['screen_size'], c='r', marker='o')\n",
    "\n",
    "ax.set_xlabel('Battery Power')\n",
    "ax.set_ylabel('Back_Camera_Px')\n",
    "ax.set_zlabel('Screen_Size')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "We are using classification task for this machine learning project. The 4 algorithms used to predit the models are:<br><br>\n",
    "1. K-Nearest Neigbors (commonly known as KNN)<br>\n",
    "2. Decision Tree<br>\n",
    "3. Random Forest<br>\n",
    "4. Support Vector Machine (also known as SVM)<br><br>\n",
    "\n",
    "Firstly,we have applied cross fold validation on the entire feature present in the dataset after pre-processing & <br>found CV score as **'0.38'**. As the score seemed to be lower,we then decided to go for feature selection to<br> see if any improvement in the accuracy.<br> We applied 'f-score' & 'random forest importance' methods for feature selection and compared the accuracy of both.<br> Finally we ended up with f-score as best features estimator and used it for further analysis.<br><br>\n",
    "We then applied the above mentioned algorithms on the best features given by f-score method and estimated the<br> accuracy of all models. Also for each algorithm, we tuned the parameters and visualised to get the best accuracy<br> score for corresponding model.<br>\n",
    "Lastly we evaluated the algorithms using the performance metrics and performance comparison using paired t-test\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Feature selection is the process where you automatically select features which contribute most to your prediction<br> variable. Sometimes having many features can decrease the accuracy of model.<br><br>\n",
    "\n",
    "1. Performance with full sets of features:<br><br>\n",
    "We first accessed the performance using all the features of our data. We used Stratified-K-fold methods with <br>splits = '5' and repetitions ='3' with scoring metric set to accuracy & lastly computed the result using <br>cross_val_score () .\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold\n",
    "\n",
    "Data= df.drop(columns=['price_range'])\n",
    "target = df[TARGET]\n",
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=786)\n",
    "scoring_metric = 'accuracy'\n",
    "cv_results_full = cross_val_score(estimator=clf, X=Data, y=target, cv=cv_method,scoring=scoring_metric)\n",
    "\n",
    "cv_results_full.mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With full set of features & 1 neigbor classifier, we achieved the accuracy score of **38%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature selection using f-score:<br><br>\n",
    "F-score method selects the features based on relationship between descriptive feature and target feature using<br>F-distribution. We now set number of features to 8. The fs_indices_fscore returns us top 8 features sorted<br>highest to lowest. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df.drop(columns=['price_range'])\n",
    "target = df[TARGET]\n",
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection as fs\n",
    "num_features = 8\n",
    "fs_fit_fscore = fs.SelectKBest(fs.f_classif, k=num_features)\n",
    "fs_fit_fscore.fit_transform(Data, target)\n",
    "fs_indices_fscore = np.argsort(np.nan_to_num(fs_fit_fscore.scores_))[::-1][0:num_features]\n",
    "fs_indices_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_fscore = df.columns[fs_indices_fscore].values\n",
    "best_features_fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We got *'ram'*, *'battery_power'*, *'int_memory'*, *'clock_speed'*, *'screen_size'*, *'n_cores'*, *'talk_time'*and <br>*'front_cam_mp'* as best features based on F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_fscore = fs_fit_fscore.scores_[fs_indices_fscore]\n",
    "feature_importances_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def plot_imp(best_features, scores, method_name, color):\n",
    "    \n",
    "    df = pd.DataFrame({'features': best_features, \n",
    "                       'importances': scores})\n",
    "    \n",
    "    chart = alt.Chart(df, \n",
    "                      width=500, \n",
    "                      title=method_name + ' Feature Importances'\n",
    "                     ).mark_bar(opacity=0.75, \n",
    "                                color=color).encode(\n",
    "        alt.X('features', title='Feature', sort=None, axis=alt.AxisConfig(labelAngle=45)),\n",
    "        alt.Y('importances', title='Importance')\n",
    "    )\n",
    "    \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plotting the best_features_fscores to visualised the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imp(best_features_fscore, feature_importances_fscore, 'F-Score', 'red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accessing the performance of the selected features using cross validation. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_fscore = cross_val_score(estimator=clf,\n",
    "                             X=Data[:, fs_indices_fscore],\n",
    "                             y=target, \n",
    "                             cv=cv_method, \n",
    "                             scoring=scoring_metric)\n",
    "cv_results_fscore.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Feature selection using Random Forest Importance<br><br>\n",
    "Random Forest importance (RFI) is widely used feature selector because of the accuracy, robustness and <br>ease of use it gives. It tells us about how much accuracy is decreased when a variable is excluded <br>and decrease in gini impurity when a variable is chosen to split node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data= df.drop(columns=['price_range'])\n",
    "target=df[TARGET]\n",
    "Data=preprocessing.MinMaxScaler().fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rfi = RandomForestClassifier(n_estimators=100)\n",
    "model_rfi.fit(Data, target)\n",
    "fs_indices_rfi = np.argsort(model_rfi.feature_importances_)[::-1][0:num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_rfi = df.columns[fs_indices_rfi].values\n",
    "best_features_rfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We got *'ram'*, *'battery_power'*, *'screen_size'*,*'int_memory'*, *'talk_time'*, *'front_cam_mp'*,<br>*'back_cam_mp'* and *'n_cores'*,as best features based on random forest importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_rfi = model_rfi.feature_importances_[fs_indices_rfi]\n",
    "feature_importances_rfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plotting the best_features_rfi to visualised the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imp(best_features_rfi, feature_importances_rfi, 'Random Forest', 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accessing the performance of the selected features using cross validation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_rfi = cross_val_score(estimator=clf,\n",
    "                             X=Data[:, fs_indices_rfi],\n",
    "                             y=target, \n",
    "                             cv=cv_method, \n",
    "                             scoring=scoring_metric)\n",
    "cv_results_rfi.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the overall performance:<br>\n",
    "We found that F-score feature selector gives us good accuracy score as compared to random forest importance.<br> Hence we choose '{best_feature_f-score}' for further fitting the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full Set of Features:', cv_results_full.mean().round(3))\n",
    "print('F-Score:', cv_results_fscore.mean().round(3))\n",
    "print('RFI:', cv_results_rfi.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and test set\n",
    "\n",
    "We have selected the subset(1200) of our entire data i.e(2000) for model fitting and evaluation.<br>We have split the data into 70 :30 ratio . i.e  70% of our data to build a model and 30% data to test it to ensure<br> that we measure the accuracy based on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df[best_features_fscore].copy()\n",
    "target = df[TARGET]\n",
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "D_train, D_test, t_train, t_test = train_test_split(Data, target, test_size=0.3, random_state=786)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fitting\n",
    "\n",
    "## 1.K-Nearest Neighbor (KNN)<br>\n",
    "We fit a ‘KNeighborClassifier’ with default parameter values as ‘n_neigbors =5’ and ‘P=2’. n_neigbors value is <br>the number of neigbors to be used and P=2 is the Euclidean distance metric .<br>\n",
    "The score function returns the accuracy of classifier on the test data. Accuracy is ratio of total correctly<br> predicted observations upon total number of observations.  Computed accuracy found was 59.44%<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, p=2)\n",
    "knn_classifier.fit(D_train, t_train) \n",
    "knn_classifier.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning using Grid Search\n",
    "Grid-search is used to find the optimal hyperparameters of a model which results in the most ‘accurate’ predictions.<br>\n",
    "\n",
    "Below we have defined a function for *'grid search'* to which we pass the classifier (KNN, DT, RF, and SVM) and<br> training data.\n",
    "* We have defined different parameters for each algorithm in the ‘grid_params’ method. \n",
    "* The function returns us best model parameters and model score based on the parameters given.\n",
    "* In addition we include repeated stratified cv method.\n",
    "* Also we tell sklean library which metric to optimize i.e. accuracy in our case.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def grid_search(D_train, t_train, clf):\n",
    "   \n",
    "    if isinstance(clf, KNeighborsClassifier): \n",
    "        grid_params = {\n",
    "        'n_neighbors':[3, 5, 7, 9, 11, 13, 15],\n",
    "        'p':[1, 2, 3]\n",
    "        }\n",
    "    elif isinstance(clf, DecisionTreeClassifier): \n",
    "        grid_params = {\n",
    "        'criterion':['gini','entropy'],\n",
    "        'min_samples_split':[2, 3, 4],\n",
    "        'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        }\n",
    "    elif isinstance(clf, RandomForestClassifier):\n",
    "        grid_params = {\n",
    "        'n_estimators':[110, 130, 150, 200],\n",
    "        'criterion':['gini','entropy'],\n",
    "        'min_samples_split':[2, 3, 4],\n",
    "        'max_depth':[3, 4, 5]\n",
    "        \n",
    "        }\n",
    "    elif isinstance(clf, SVC):\n",
    "       grid_params = {\n",
    "            'C':[1, 10, 50, 100],\n",
    "            'gamma':[1, 0.1, 0.05, 0.001],\n",
    "            'kernel':['rbf', 'poly', 'sigmoid']\n",
    "        }\n",
    "    else : \n",
    "        raise ValueError(\"unkown classifier\")\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator = clf,\n",
    "        param_grid = grid_params,\n",
    "        verbose = 3,\n",
    "        cv = cv_method,\n",
    "        n_jobs = -1,\n",
    "        refit = True   \n",
    "    )\n",
    "\n",
    "    gs_results = gs.fit(D_train, t_train)\n",
    "    p = gs_results.best_params_\n",
    "    model = gs_results.best_estimator_\n",
    "    return model, p, gs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With n_neigbors: [3, 5, 7, 9, 11, 13, 15] and P: [1, 2, 3] the grid search function finds out the best parameter <br>values and calculates the model score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model, knn_best_estimate, knn_result = grid_search(D_train, t_train, knn_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KNN classifier with n_neighbor =15 and p=1 predicted the model mean score of 68.8% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_KNN = pd.DataFrame(knn_result.cv_results_['params'])\n",
    "results_KNN['test_score'] = knn_result.cv_results_['mean_test_score']\n",
    "results_KNN.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_KNN['metric'] = results_KNN['p'].replace([1, 2, 3], [\"Manhattan\", \"Euclidean\", \"Minkowski\"])\n",
    "results_KNN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the KNN Performance comparison. \n",
    "We know visualise the hyper parameter tuning results from cross fold validation. We plot using altair module.<br> The plot shows that at all values of K with Manhattan distance (p=1) outperforms others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(results_KNN, \n",
    "          title='KNN Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('n_neighbors', title='Number of Neighbors'),\n",
    "    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n",
    "    color='metric'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of KNN Classifier:\n",
    "*   The algorithm is simple and easy to implement .\n",
    "*   The algorithm is versatile and can be used for classification , regression and search.<br><br>\n",
    "\n",
    "#### Disadvantages:\n",
    "*   The algorithm gets slower as number of independent variables increases where predictions needs to be made<br> rapidly.\n",
    "\n",
    "#### Limitations:\n",
    "*   Need to have high computing resources to speedly handle the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Decisoin Tree Clasification\n",
    "\n",
    "Decision trees are non-parametric supervised learning methods used for classification. The main aim of this is to <br> define a model that gives value of target feature by learning decision rule inferred from data features.<br><br>\n",
    "Fitting the decision tree classifier with default values and random state = 786 which was selected at the very <br>beginning.<br><br>\n",
    "The score function returns the accuracy of classifier on the test data. Accuracy is ratio of total correctly <br>predicted observations upon total number of observations.<br> The accuracy measured was 76.38%.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=786)\n",
    "dt_classifier.fit(D_train, t_train)\n",
    "dt_classifier.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Out of Criterion = [‘gini’ ,entropy’] , min_sample_split=[2, 3, 4] & max_depth=[1, 2, 3, 4, 5, 6, 7, 8] the grid <br> search function finds out the best parameter values and calculates the model score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model, dt_best_estimate, dt_result = grid_search(D_train, t_train, dt_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With Criterion ‘gini’ , max_depth =4 and min_sample_splits of 2 the model predicts the accuracy 76.6%*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_DT = pd.DataFrame(dt_result.cv_results_['params'])\n",
    "results_DT['test_score'] = dt_result.cv_results_['mean_test_score']\n",
    "results_DT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the DT Performance Comparison \n",
    "Also from the plot we visualise the best hyperparamters as ‘gini’ and ‘max_depth:4’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(results_DT, \n",
    "          title='DT Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('max_depth', title='Maximum Depth'),\n",
    "    alt.Y('test_score', title='Mean CV Score', aggregate='average', scale=alt.Scale(zero=False)),\n",
    "    color='criterion'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Advantages of Decision tree classifier\n",
    "*\tInexpensive to construct. \n",
    "*\tEasy to interpret for small size trees. \n",
    "*\tFast at classifying unknown records.\n",
    "\n",
    "#### Disadvantages\n",
    "*\tDecision tree models are often biased towards splits on features.\n",
    "*\tLarge trees can be difficult to interpret.\n",
    "*\tSmall change in training data can account for large change to decision logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Random Forest Classifier\n",
    "A random forest is a Meta estimator that fits number of decision tree classifier on various sub-samples<br> and uses mean to advance the accuracy and avoid over-fitting.<br><br>\n",
    "Fitting the random forest classifier with default estimator n= 100 i.e. number of trees in the forest,<br> criterion ‘gini and max_depth 2.<br>\n",
    "\n",
    "The score function returns the accuracy of classifier on the test data. Accuracy is ratio of total correctly predicted observations upon total number of observations. The accuracy measured was *73.6%*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=786,n_estimators=100,max_depth=2,criterion='gini')\n",
    "rf_classifier.fit(D_train, t_train)\n",
    "rf_classifier.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Out of the given parameters given to grid search function criterion =[‘gini’, ‘entropy’], <br>n_estimators= [110, 130, 150, 200], max_depth=[3, 4, 5] and min_sample_split= [2, 3, 4] it calculates & returns best <br>parameters with model score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model, rf_best_estimate, rf_result = grid_search(D_train, t_train, rf_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model predicts the accuracy score of 76.9%.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_RF = pd.DataFrame(rf_result.cv_results_['params'])\n",
    "results_RF['test_score'] = rf_result.cv_results_['mean_test_score']\n",
    "results_RF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the RF Performance Comparison \n",
    "From the plot we visualise that at max_depth =4 , gini overpowers entropy . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(results_RF, \n",
    "          title='RF Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('max_depth', title='Maximum Depth'),\n",
    "    alt.Y('test_score', title='Mean CV Score', aggregate='average', scale=alt.Scale(zero=False)),\n",
    "    color='criterion'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Random forest classifier\n",
    "*\tNo need of any feature selection \n",
    "*\tEasier to make parallel models \n",
    "*\tIf larger parts of features are lost , accuracy can still be maintained.\n",
    "\n",
    "#### Disadvantages\n",
    "*\tFits for some noisy data \n",
    "*\tTime complexity- much harder and time consuming to construct.\n",
    "\n",
    "#### Limitations\n",
    "*\tHeavy computation resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Support Vector Machine classifier \n",
    "SVM is linear model for classification problem. The idea of SVM is simple. The algorithm creates <br>a line or hyperplane which separates the data into classes.<br><br>\n",
    "We fit the model with default kernel as rbf and regularisation value=1.0 parameters .<br><br>\n",
    "The score function returns the accuracy of classifier on the test data. Accuracy is ratio of total<br> correctly predicted observations upon total number of observations. The accuracy measured was 78.6%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(D_train, t_train)\n",
    "svm_classifier.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The parameters passed to grid search function were gamma values=[0.1, 0.05, 0.001, 1] as the value <br> must be between 0.1 to 1. Kernels=[‘rbf’, ‘poly’, ‘sigmoid’] with C=[1, 10, 50, 100]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model, svm_best_estimate, svm_result = grid_search(D_train, t_train, svm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.score(D_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The model predicts the accuracy score of 83.4% with best parameters .*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_SVM = pd.DataFrame(svm_result.cv_results_['params'])\n",
    "results_SVM['test_score'] = svm_result.cv_results_['mean_test_score']\n",
    "results_SVM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the SVM Performance Comparison \n",
    "From the plot we visualise that at max_depth =4 , gini overpowers entropy . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(results_SVM, \n",
    "          title='SVM Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('C', title='Regularisation Parameter'),\n",
    "    alt.Y('test_score', title='Mean CV Score', aggregate='average', scale=alt.Scale(zero=False)),\n",
    "    color='kernel'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Support Vector Machine\n",
    "*\tWork well when there is clean margin of separation.\n",
    "*\tMemory efficient\n",
    "#### Disadvantages\n",
    "*\tNot suitable for larger data sets\n",
    "*\tSVM does not perform well when data set has more noise or target class is overlapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "After testing the classifier  by considering the train data  and using it in cross validation way, <br>we know perform the paired t-test in order to understand if the difference between performance is statistically<br> significant for any 2 classifiers.<br><br>\n",
    "Firstly we calculate the cross_val_score  and then compare it with all models as:\n",
    "*\tKNN-DT\n",
    "*\tKNN-RF\n",
    "*\tKNN-SVM\n",
    "*\tDT-RF\n",
    "*\tDT-SVM\n",
    "*\tRF_SVM<br>\n",
    "\n",
    "From scipy library we import the stats module to run the t-test .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "cv_method_ttest = StratifiedKFold(n_splits=10, random_state=786)\n",
    "cv_results_KNN = cross_val_score(estimator=knn_model, X=Data, y=target, cv=cv_method_ttest, n_jobs=-1, scoring='accuracy')              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_KNN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_RF = cross_val_score(estimator=rf_model, X=Data, y=target, cv=cv_method_ttest, n_jobs=-1, scoring='accuracy')\n",
    "cv_results_RF.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_DT = cross_val_score(estimator=dt_model, X=Data, y=target, cv=cv_method_ttest, n_jobs=-1, scoring='accuracy')\n",
    "cv_results_DT.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_SVM = cross_val_score(estimator=svm_model, X=Data, y=target, cv=cv_method_ttest, n_jobs=-1, scoring='accuracy')\n",
    "cv_results_SVM.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.ttest_rel(cv_results_KNN, cv_results_DT))\n",
    "print(stats.ttest_rel(cv_results_KNN, cv_results_RF))\n",
    "print(stats.ttest_rel(cv_results_KNN, cv_results_SVM))\n",
    "\n",
    "print(stats.ttest_rel(cv_results_DT, cv_results_RF))\n",
    "print(stats.ttest_rel(cv_results_DT, cv_results_SVM))\n",
    "\n",
    "print(stats.ttest_rel(cv_results_RF, cv_results_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Pair KNN-SVM gives statistically significant value of 0.0002 which is less than 0.05.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation \n",
    "Model evaluation is one of the important step required to determine the best model, how well the model will perform.<br><br>\n",
    "The target variable for our dataset was multinomial. That is target feature is categorical with 4 different <br>level {0, 1, 2, 3}. It refers to different price range ={‘low’, ‘mid’, ‘high’, ‘v high’}. <br>Hence we cannot use binary metric such as roc_auc curve to evaluate multinomial classifier.<br><br>\n",
    "Below are the evaluation metrics used to find the accuracy, classification report and average model accuracy<br> for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def print_model_stats(model, D_test, t_test):\n",
    "    pred = model.predict(D_test)\n",
    "    print(\"=========={model_name} Model Statistics=============\".format(model_name=model.__class__.__name__))\n",
    "    print(\"Accuracy score:\", metrics.accuracy_score(t_test, pred))\n",
    "    print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(t_test, pred))\n",
    "    print(\"Classification report:\\n\", metrics.classification_report(t_test, pred))\n",
    "    print(\"Average model accuracy:\", metrics.balanced_accuracy_score(t_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(knn_model, D_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(dt_model, D_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(rf_model, D_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(svm_model, D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hence we conclude that SVM gives us the best model accuracy and should be used for this case study.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.2 32-bit",
   "language": "python",
   "name": "python37232bitc1863d35bc9042759fff512d33211ec4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}