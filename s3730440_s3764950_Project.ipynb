{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Project - MATH2319 Machine Learning | Sem 1, 2020\n",
    "## Honour Code\n",
    "\n",
    "We solemnly swear that we have not discussed our assignment solutions with anyone in any way and the solutions we are submitting are our own personal work.\n",
    "\n",
    "Full Name: Ankit Munot (s3764950), Akshay Sunil Salunke (s3730440) | Group 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [Objective](#1)\n",
    "* [Source and description of dataset](#2)\n",
    "  - [Dataset](#2.1)\n",
    "  - [Target feature](#2.2)\n",
    "  - [Descriptive features](#2.3)\n",
    "* [Data preprocessing](#3)\n",
    "  - [Preliminaries](#3.1)\n",
    "  - [Data cleaning and transformation](#3.2)\n",
    "  - [Summary of features](#3.3)\n",
    "* [Data Exploration and Visualisation](#4)\n",
    "  - [Univariate visualizations](#4.1)\n",
    "  - [Bivariate visualizations](#4.2)\n",
    "  - [Multivariate visualizations](#4.3)\n",
    "* [Methodology](#5)\n",
    "  - [Feature selection](#5.1)\n",
    "  * [Model fitting](#5.2)\n",
    "    - [K-Nearest Neighbor (KNN)](#5.2.1)\n",
    "    - [Decision Tree Classifier](#5.2.2)\n",
    "    - [Random Forest Classifier](#5.2.3)\n",
    "    - [Support Vector Machine](#5.2.4)\n",
    "  * [Performance Comparison](#5.3)\n",
    "* [Model Evaluation](#6)\n",
    "* [Summary and Conclusion](#7)\n",
    "* [References](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "The objective of this project is to predict mobile phone price range, based on various features a phone has. We predict price for a phone as a price-bucket rather than as a continuos feature, hence the problem at hand is multinomial classification problem. We have performed following steps in this project, data visualization, data preprocessing,  feature selection (using f-score and random forest), model evaluation (4 models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source and description of dataset\n",
    "## Dataset\n",
    "The dataset for this project was sourced from [kaggle.com](https://www.kaggle.com/iabhishekofficial/mobile-price-classification). A copy of dataset is included in `/data` folder, as accessed on `26 May 2020`. The dataset originally has 2,000 rows, but we have randomly sampled 1200 rows so that our laptops could cope up.\n",
    "The dataset has 20 features(excluding target), some of which are binary (eg. `wifi`, `bluetooth`, etc.) and denote if a phone has that particular feature, whereas some are continuos features (eg. `batter_power`, `ram`, etc.)\n",
    "\n",
    "## Target feature\n",
    "The target feature for this dataset is `price_range`, which has `0,1,2,3` as possible values, which represents phone price category corresponding to `low, mid, high, v.high`\n",
    "\n",
    "## Descriptive features\n",
    "The dataset has followinf descriptive features:\n",
    "- `battery_power`: Total energy a battery can store in one time measured in *mAh.*\n",
    "- `bluetooth`: Has bluetooth or not.\n",
    "- `clock_speed`: Speed at which microprocessor executes instructions in *GHz*.\n",
    "- `dual_sim`: Has dual sim support or not.\n",
    "- `front_cam_mp`: Front Camera mega pixels.\n",
    "- `four_g`: Has 4G or not.\n",
    "- `int_memory`: Internal Memory in *Gigabytes*.\n",
    "- `n_cores`: Number of cores of processor.\n",
    "- `back_cam_mp`: Primary Camera in *Megapixels*.\n",
    "- `px_height`: Pixel Resolution Height in *pixels*.\n",
    "- `px_width`: Pixel Resolution Width in *pixels*.\n",
    "- `ram`: Random Access Memory in *Megabytes*.\n",
    "- `sc_h`: Screen Height of mobile in *cm*.\n",
    "- `sc_w`: Screen Width of mobile in *cm*.\n",
    "- `talk_time`: longest time that a single battery charge will last when you are on a call, in *Hours*.\n",
    "- `three_g`: Has 3G or not.\n",
    "- `touch_screen`: Has touch screen or not.\n",
    "- `wifi`: Has wifi or not.\n",
    "- `screen_size`: Screen size diagonally in *inches*.\n",
    "\n",
    "Some of the features have been transformed into new features, for example, `sc_h` and `sc_w` have been transformed into `screen_size`, which is screen size measured diagonally in inches, which is the industry standard for measuring phone screen sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "## Preliminaries\n",
    "We import relevant datascience libraries, which we could think off top of our heads. Also, ignore the warning(because they're annoying)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "import warnings \n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now read the data in an pandas dataframe. We randomly sample 1200 rows, with a fixed `random_state` so that results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/train.csv\")\n",
    "df = df.sample(1200, random_state=786)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how the original dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is it's shape:  (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and transformation\n",
    "In this section we remove the irrelevant features, transform existing features into new, etc.\n",
    "\n",
    "We now check for any null values right off the bat, in all `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But isnull() only detects those values which are `NaN` or `None`. This is not enough and we still need to check the `df` for missing values in other formats, for example `?` or `0`.\n",
    "\n",
    "We first rename the columns to more readable and understandable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'blue':'bluetooth', 'fc':'front_cam_mp', 'sc_h':'screen_ht', 'sc_w':'screen_wt', 'pc':'back_cam_mp'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we drop the columns like `mobile_wt` and `m_dep` which we think are not actual deciding factors for a phone price, but more of a byproduct after the phone has been already manufactured.\n",
    "We also dropped `px_height` and `px_width` since it played very little to no role as a deciding factor for price of a phone, for the general consumer, since most of the consumers focus on the physical screen size rather than how many pixels a display has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['m_dep', 'mobile_wt', 'px_height', 'px_width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform binning of continuos feature `clock_speed`, into 3-bins namely, `low, mid, high` which correspond to following buckets `0-1, 1-2, 2-3`, in *Ghz*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clock_speed'] = pd.cut(df['clock_speed'], bins=[0, 1, 2, 3], labels = ['low', 'mid', 'high'])\n",
    "level_map = {'low':0, 'mid':1, 'high':2}\n",
    "df['clock_speed'] = df['clock_speed'].replace(level_map)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also transform the features `screen_wt` and `screen_ht` which are in *cm*, to new feature `screen_size` which is the diagonal screen size in *inches*, which is the industry standard for measuring screen sizes of phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['screen_ht', 'screen_wt']].describe()\n",
    "\n",
    "# Convert column datatypes to float.\n",
    "df['screen_ht'] = df['screen_ht'].astype(float)\n",
    "df['screen_wt'] = df['screen_wt'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first handle the missing values in `screen_wt`. For this, we find the height for missing width, calculate mean width for that height in whole df, then append this mean to missijg value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temp list of all screen heights whose screen width is 0.\n",
    "x = df[df['screen_wt']==0]['screen_ht'].value_counts().index.tolist()\n",
    "\n",
    "# Create a list of all screen heights whose screen width is 0.\n",
    "arr = []\n",
    "for d in df.loc[df['screen_wt']==0]['screen_ht']:\n",
    "    arr.append(d)\n",
    "\n",
    "# Calculate mean width for a specific screen height. \n",
    "# We create set out of screen heights list so that width is calculated only for unique screen heights.\n",
    "# mean_width stores all unique screen heights, and their mean width.\n",
    "mean_width = {}\n",
    "for d in set(arr):\n",
    "    total = 0\n",
    "    n = 0\n",
    "    for width in df.loc[df['screen_ht'] == d]['screen_wt']:\n",
    "        if width == 0:\n",
    "            pass\n",
    "        total += width\n",
    "        n += 1\n",
    "        mean = round(total/n, 2)\n",
    "    print(\"Mean width for height\", d, \"=\", mean)\n",
    "    mean_width[d] = mean\n",
    "\n",
    "# Append all missing screen widths with the mean screen width for that specific height.\n",
    "for z in x:\n",
    "    df['screen_wt'] = np.where(((df['screen_wt']==0.0) & (df['screen_ht']==z)), mean_width.get(z), df['screen_wt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now transform the `screen_height` and `screen_width` features to `screen_size` feature.\n",
    "Using the formula for diagonal of a rectangle = $\\sqrt{width^2 + heigth^2}$. Then we convert to *inches* by dividing with 2.54."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['screen_size'] = df['screen_ht']**2 + df['screen_wt']**2\n",
    "df['screen_size'] = np.sqrt(df['screen_size'])\n",
    "df['screen_size'] = df['screen_size']/2.54\n",
    "df['screen_size'] = df['screen_size'].round(2)\n",
    "df.drop(columns=['screen_ht', 'screen_wt'], inplace=True)\n",
    "\n",
    "p = pd.DataFrame(df['price_range'])\n",
    "df.drop(columns=['price_range'], inplace=True)\n",
    "df = df.join(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also, seperate out the categorical features, continuos features and the target feature in seperate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['bluetooth', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']\n",
    "continuous_features = ['battery_power', 'front_cam_mp', 'int_memory', 'n_cores', 'back_cam_mp', 'ram', 'clock_speed', 'talk_time']\n",
    "TARGET = ['price_range']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of features\n",
    "We describe the continuos features, to understand the dataset and draw some insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[continuous_features].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the shape of our `df` after dropping unrequired columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now print unique values for all features to find any missing/outlier values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bluetooth'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dual_sim'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['front_cam_mp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['back_cam_mp'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the `0` values in `from_cam_mp` and `back_cam_mp` are not outliers and instead mean that those phone lack that specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['four_g'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['int_memory'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extreme values for `int_memory` are 2Gb and 64Gb, which are both available as internam memory on phones. Hence there are no outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['n_cores'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ram'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['talk_time'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['three_g'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['touch_screen'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['four_g'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['wifi'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_range'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All categorical values have possible values, no missing values or outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Visualisation\n",
    "In this section we visualize the data and try to get some insights from it.\n",
    "## Univariate visualizations\n",
    "In this section we try to visualize and analyze one feature at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we plot a bar plot of counts of `n_cores` across the whole dataset. There is no clear pattern visible, but you learn that some number of cores appear more frequently (like `4`, `7`, `8`), denoting that these core sizes occur more frequently, which aligns with the fact that quad and octa core are standard # of cores for many major processor brands (like Qualcomm and Mediatek)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar Plot\n",
    "fig = plt.figure(figsize = (6, 4))\n",
    "title = fig.suptitle(\"No. of Cores\", fontsize = 14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel(\"No.of Cores\")\n",
    "ax.set_ylabel(\"Count #\") \n",
    "w_q = df['n_cores'].value_counts()\n",
    "w_q = (list(w_q.index), list(w_q.values))\n",
    "ax.tick_params(axis='both', which='major', labelsize=8.5)\n",
    "bar = ax.bar(w_q[0], w_q[1], color='steelblue', \n",
    "        edgecolor='black', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot a pie chart on `clock_speed` feature, which shows that `low` clock speed appears the most, which suggests that majority of the phones operated on a processor which was between `0-1 Ghz`, followed by `1-2 Ghz` and lastly the high end phones with `2-3 Ghz` clock speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['low', 'mid', 'high']\n",
    "df['clock_speed'].value_counts().plot(kind='pie', autopct='%.2f')\n",
    "plt.tight_layout()\n",
    "plt.legend(labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `screen_size` is a continuos feature, we plot a density graph, to try to find which screen sizes were more prominent. We find that screen size across phones can be split into 2 distinct different categories, which aligns with today's trend of smaller 4.7 *inches* screens and larger 6+ *inches* screens. (Apple iPhone SE 2020 - 4.7 *inches*, iPhone 11 Pro Max - 6.5 *inches*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['screen_size'].plot(kind='density', title=\"Screen size density plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bivariate visualizations\n",
    "In this section we try to visualize and analyze two features at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with plotting a 2d histogram for `ram` and `price_range`. We can clearly see the density of ram for each price range. Following are the average ram sizes for each price range:\n",
    "- `low` : 0 - 750 MB\n",
    "- `mid` : 1250 - 1800 MB\n",
    "- `high` : ~2500 MB\n",
    "- `v.high` : 3500 - 4000 MB\n",
    "\n",
    "We can clearly see the pattern, as price range increases, average ram size also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['low', 'mid', 'high', 'v.high']\n",
    "h, x, y, i = plt.hist2d(df['price_range'], df['ram'], bins=(4, 16), cmap='Blues')\n",
    "bin_w = (max(x) - min(x)) / (len(x) - 1)\n",
    "plt.xticks(np.arange(min(range(0,4))+bin_w/2, max(range(0, 4)), bin_w), labels)\n",
    "plt.xlabel(\"Price range vs. Ram\")\n",
    "plt.ylabel(\"RAM size in MB\")\n",
    "cb = plt.colorbar(i)\n",
    "cb.set_label('ram counts in bin')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot a bar graph between `dual_sim` and `talk_time` features. Even though there is no clear pattern visible, we can see that talk time for non dual sim phones falls around the higher end and vice versa. From this, we can assume that dual sim phones have less talk time on average as compared to non dual sim phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using subplots or facets along with Bar Plots\n",
    "fig = plt.figure(figsize = (10, 4))\n",
    "title = fig.suptitle(\"Dual_sim vs. talk_time\", fontsize=14)\n",
    "fig.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "# Non Dual Sim\n",
    "ax1 = fig.add_subplot(1,2, 1)\n",
    "ax1.set_title(\"Non Dual Sim\")\n",
    "ax1.set_xlabel(\"Talk-Time in Hrs.\")\n",
    "ax1.set_ylabel(\"Frequency\") \n",
    "rw_q = df[df['dual_sim'] == 0]['talk_time'].value_counts()\n",
    "rw_q = (list(rw_q.index), list(rw_q.values))\n",
    "ax1.set_ylim([0,70])\n",
    "ax1.tick_params(axis='both', which='major', labelsize=8.5)\n",
    "bar1 = ax1.bar(rw_q[0], rw_q[1], color='red', \n",
    "               edgecolor='black', linewidth=1)\n",
    "\n",
    "# Dual Sim\n",
    "ax2 = fig.add_subplot(1,2, 2)\n",
    "ax2.set_title(\"Dual Sim\")\n",
    "ax2.set_xlabel(\"Talk-time in Hrs.\")\n",
    "ax2.set_ylabel(\"Frequency\") \n",
    "ww_q = df[df['dual_sim'] == 1]['talk_time'].value_counts()\n",
    "ww_q = (list(ww_q.index), list(ww_q.values))\n",
    "ax2.set_ylim([0, 70])\n",
    "ax2.tick_params(axis='both', which='major', labelsize=8.5)\n",
    "bar2 = ax2.bar(ww_q[0], ww_q[1], color='white', \n",
    "               edgecolor='black', linewidth=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plot a boxplot for `battery_power` vs `price_range`. Here we can see that as the price range increases, battery capacity also tends to increase. But, this is not true for `mid` and `high` range phones. High range phones have very little battery capacity increase as compared to `mid` range phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(column='battery_power', by='price_range', figsize=(6,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate visualizations\n",
    "In this section we try to visualize and analyze three or more features at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we plot 5 different features with each other in a pair plot, to find any dependence/pattern between features. Following observations can be concluded from the pair plot:\n",
    "- `ram` feature shows distinct seperation along target feature, meaning the average ram differs most between price range.\n",
    "- `low` and `v.high` prices phones tend to have lower or higher internal memory respectively. But `mid` and `high` priced phones can have a lot of options avaiable for internal memory sizes.\n",
    "- all phones tend to have bigger `screen_size`, except `high` priced phones. This may mean that there are people who tend to purchase high priced phones but are looking for smaller screen sizes.\n",
    "- Only `v.high` priced phones can offer higher battery capacity in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling attribute values to avoid few outiers\n",
    "cols = ['ram', 'int_memory', 'screen_size', 'battery_power','price_range']\n",
    "pp = sns.pairplot(df[cols], hue='price_range', size=1.8, aspect=1.8, \n",
    "                  palette={0: \"#FF9999\", 1: \"#FFE888\", 2:\"#2A9D8F\", 3:\"#E63946\"},\n",
    "                  plot_kws=dict(edgecolor=\"black\", linewidth=0.5))\n",
    "fig = pp.fig \n",
    "fig.subplots_adjust(top=0.93, wspace=0.3)\n",
    "t = fig.suptitle('Wine Attributes Pairwise Plots', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we plot a relationship plot for `int_memory` vs. `four_g` vs. `front_cam_mp`. Here we can see that as front camera megapixels increases, internal memory of phones also tends to increase, presumably to cope with the multimedia possibilites whoch are opened with a good camera module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(y=\"int_memory\", x=\"front_cam_mp\", hue='four_g', kind=\"line\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(df['battery_power'], df['back_cam_mp'], df['screen_size'], c='orange')\n",
    "\n",
    "ax.set_xlabel('Battery Power')\n",
    "ax.set_ylabel('Back_Camera_Px')\n",
    "ax.set_zlabel('Screen_Size')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "We are using classification task for this machine learning project. The 4 algorithms used to predit the models are:<br><br>\n",
    "1. K-Nearest Neigbors (commonly known as KNN)<br>\n",
    "2. Decision Tree<br>\n",
    "3. Random Forest<br>\n",
    "4. Support Vector Machine (also known as SVM)<br><br>\n",
    "\n",
    "Firstly,we have applied cross fold validation on the entire feature present in the dataset after pre-processing & found CV score as **'0.38'**. As the score seemed to be lower,we then decided to go for feature selection to see if any improvement in the accuracy. We applied `f-score` & `random forest importance` methods for feature selection and compared the accuracy of both. Finally we ended up with f-score as best features estimator and used it for further analysis.\n",
    "\n",
    "\n",
    "We then applied the above mentioned algorithms on the best features given by f-score method and estimated the accuracy of all models. Also for each algorithm, we tuned the parameters and visualised to get the best accuracy score for corresponding model.\n",
    "\n",
    "Lastly we evaluated the algorithms using the performance metrics and performance comparison using paired t-test\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection\n",
    "Feature selection is the process where you automatically select features which contribute most to your prediction variable. Sometimes having many features can decrease the accuracy of model.<br><br>\n",
    "\n",
    "1. Performance with full sets of features:\n",
    "We first accessed the performance using all the features of our data. We used `Stratified-K-fold` methods with `splits = 5` and `repetitions = 3` with scoring metric set to accuracy & lastly computed the result using `cross_val_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score,RepeatedStratifiedKFold\n",
    "\n",
    "Data= df.drop(columns=['price_range'])\n",
    "target = df[TARGET]\n",
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "cv_method = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=786)\n",
    "scoring_metric = 'accuracy'\n",
    "cv_results_full = cross_val_score(estimator=clf, X=Data, y=target, cv=cv_method,scoring=scoring_metric)\n",
    "\n",
    "cv_results_full.mean().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With full set of features & `1` neigbor classifier, we achieved the accuracy score of **38%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature selection using f-score:<br><br>\n",
    "F-score method selects the features based on relationship between descriptive feature and target feature using F-distribution. We now set number of features to `8`. The `fs_indices_fscore` returns us top 8 features sorted highest to lowest. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df.drop(columns=['price_range'])\n",
    "target = df[TARGET]\n",
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_selection as fs\n",
    "num_features = 8\n",
    "fs_fit_fscore = fs.SelectKBest(fs.f_classif, k=num_features)\n",
    "fs_fit_fscore.fit_transform(Data, target)\n",
    "fs_indices_fscore = np.argsort(np.nan_to_num(fs_fit_fscore.scores_))[::-1][0:num_features]\n",
    "fs_indices_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_fscore = df.columns[fs_indices_fscore].values\n",
    "best_features_fscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We got `ram, battery_power, int_memory, clock_speed, screen_size, n_cores, talk_time` and `front_cam_mp` as best features based on F-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_fscore = fs_fit_fscore.scores_[fs_indices_fscore]\n",
    "feature_importances_fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def plot_imp(best_features, scores, method_name, color):\n",
    "    \n",
    "    df = pd.DataFrame({'features': best_features, \n",
    "                       'importances': scores})\n",
    "    \n",
    "    chart = alt.Chart(df, \n",
    "                      width=500, \n",
    "                      title=method_name + ' Feature Importances'\n",
    "                     ).mark_bar(opacity=0.75, \n",
    "                                color=color).encode(\n",
    "        alt.X('features', title='Feature', sort=None, axis=alt.AxisConfig(labelAngle=45)),\n",
    "        alt.Y('importances', title='Importance')\n",
    "    )\n",
    "    \n",
    "    return chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plotting the best_features_fscores to visualised the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imp(best_features_fscore, feature_importances_fscore, 'F-Score', 'red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accessing the performance of the selected features using cross validation. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_fscore = cross_val_score(estimator=clf,\n",
    "                             X=Data[:, fs_indices_fscore],\n",
    "                             y=target, \n",
    "                             cv=cv_method, \n",
    "                             scoring=scoring_metric)\n",
    "cv_results_fscore.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Feature selection using Random Forest Importance<br><br>\n",
    "Random Forest importance (RFI) is widely used feature selector because of the accuracy, robustness and <br>ease of use it gives. It tells us about how much accuracy is decreased when a variable is excluded <br>and decrease in gini impurity when a variable is chosen to split node.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data= df.drop(columns=['price_range'])\n",
    "target=df[TARGET]\n",
    "Data=preprocessing.MinMaxScaler().fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_rfi = RandomForestClassifier(n_estimators=100)\n",
    "model_rfi.fit(Data, target)\n",
    "fs_indices_rfi = np.argsort(model_rfi.feature_importances_)[::-1][0:num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features_rfi = df.columns[fs_indices_rfi].values\n",
    "best_features_rfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We got `ram`, `battery_power`, `screen_size`,`int_memory`, `talk_time`, `front_cam_mp`, `back_cam_mp` and `n_cores`,as best features based on random forest importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_rfi = model_rfi.feature_importances_[fs_indices_rfi]\n",
    "feature_importances_rfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plotting the `best_features_rfi` to visualise the feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_imp(best_features_rfi, feature_importances_rfi, 'Random Forest', 'green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accessing the performance of the selected features using cross validation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_rfi = cross_val_score(estimator=clf,\n",
    "                             X=Data[:, fs_indices_rfi],\n",
    "                             y=target, \n",
    "                             cv=cv_method, \n",
    "                             scoring=scoring_metric)\n",
    "cv_results_rfi.mean().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the overall performance:\n",
    "We found that F-score feature selector gives us good accuracy score as compared to random forest importance. \n",
    "\n",
    "Hence we choose `best_feature_f-score` for further fitting the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Full Set of Features:', cv_results_full.mean().round(3))\n",
    "print('F-Score:', cv_results_fscore.mean().round(3))\n",
    "print('RFI:', cv_results_rfi.mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into training and test set\n",
    "\n",
    "We have selected the sample(1200) of our entire data i.e(2000 rows) for model fitting and evaluation. We have split the data into `70 :30` ratio i.e 70% of our data to build a model and 30% data to test it to ensure that we measure the accuracy based on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df[best_features_fscore].copy()\n",
    "target = df[TARGET]\n",
    "Data = preprocessing.MinMaxScaler().fit_transform(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "D_train, D_test, t_train, t_test = train_test_split(Data, target, test_size=0.3, random_state=786)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting\n",
    "\n",
    "## 1.K-Nearest Neighbor (KNN)<br>\n",
    "We fit a `KNeighborClassifier` with default parameter values as `n_neigbors = 5` and `P=2`. n_neigbors value is the number of neigbors to be used and `P=2` is the Euclidean distance metric.\n",
    "The score function returns the accuracy of classifier on the test data. Accuracy is ratio of total correctly predicted observations upon total number of observations. Computed accuracy found was 59.44%<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5, p=2)\n",
    "knn_classifier.fit(D_train, t_train) \n",
    "knn_classifier.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning using Grid Search\n",
    "Grid-search is used to find the optimal hyperparameters of a model which results in the most *accurate* predictions.<br>\n",
    "\n",
    "Below we have defined a function for `grid search` to which we pass the classifier (KNN, DT, RF, and SVM) and training data.\n",
    "* We have defined different parameters for each algorithm in the `grid_params` method. \n",
    "* The function returns us best model parameters and model score based on the parameters given.\n",
    "* In addition we include repeated stratified cv method.\n",
    "* Also we tell sklean library which metric to optimize i.e. accuracy in our case.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def grid_search(D_train, t_train, clf):\n",
    "   \n",
    "    if isinstance(clf, KNeighborsClassifier): \n",
    "        grid_params = {\n",
    "        'n_neighbors':[3, 5, 7, 9, 11, 13, 15],\n",
    "        'p':[1, 2, 3]\n",
    "        }\n",
    "    elif isinstance(clf, DecisionTreeClassifier): \n",
    "        grid_params = {\n",
    "        'criterion':['gini','entropy'],\n",
    "        'min_samples_split':[2, 3, 4],\n",
    "        'max_depth':[1, 2, 3, 4, 5, 6, 7, 8]\n",
    "        }\n",
    "    elif isinstance(clf, RandomForestClassifier):\n",
    "        grid_params = {\n",
    "        'n_estimators':[110, 130, 150, 200],\n",
    "        'criterion':['gini','entropy'],\n",
    "        'min_samples_split':[2, 3, 4],\n",
    "        'max_depth':[3, 4, 5]\n",
    "        \n",
    "        }\n",
    "    elif isinstance(clf, SVC):\n",
    "       grid_params = {\n",
    "            'C':[1, 10, 50, 100],\n",
    "            'gamma':[1, 0.1, 0.05, 0.001],\n",
    "            'kernel':['rbf', 'poly', 'sigmoid']\n",
    "        }\n",
    "    else : \n",
    "        raise ValueError(\"unkown classifier\")\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator = clf,\n",
    "        param_grid = grid_params,\n",
    "        verbose = 3,\n",
    "        cv = cv_method,\n",
    "        n_jobs = -1,\n",
    "        refit = True   \n",
    "    )\n",
    "\n",
    "    gs_results = gs.fit(D_train, t_train)\n",
    "    p = gs_results.best_params_\n",
    "    model = gs_results.best_estimator_\n",
    "    return model, p, gs_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With `n_neigbors:[3, 5, 7, 9, 11, 13, 15]` and `P: [1, 2, 3]` the grid search function finds out the best parameter values and calculates the model score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model, knn_best_estimate, knn_result = grid_search(D_train, t_train, knn_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* KNN classifier with `n_neighbor = 15` and `p = 1` predicted the model mean score of 68.8% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_KNN = pd.DataFrame(knn_result.cv_results_['params'])\n",
    "results_KNN['test_score'] = knn_result.cv_results_['mean_test_score']\n",
    "results_KNN.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_KNN['metric'] = results_KNN['p'].replace([1, 2, 3], [\"Manhattan\", \"Euclidean\", \"Minkowski\"])\n",
    "results_KNN.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the KNN Performance comparison. \n",
    "We know visualise the hyper parameter tuning results from cross fold validation. We plot using altair module.<br> The plot shows that at all values of `K` with Manhattan distance `p=1` outperforms others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(results_KNN, \n",
    "          title='KNN Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('n_neighbors', title='Number of Neighbors'),\n",
    "    alt.Y('test_score', title='Mean CV Score', scale=alt.Scale(zero=False)),\n",
    "    color='metric'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of KNN Classifier:\n",
    "*   The algorithm is simple and easy to implement.\n",
    "*   The algorithm is versatile and can be used for classification, regression and search.<br><br>\n",
    "\n",
    "#### Disadvantages:\n",
    "*   The algorithm gets slower as number of independent variables increases where predictions needs to be made<br> rapidly.\n",
    "\n",
    "#### Limitations:\n",
    "*   Need to have high computing resources to speedly handle the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Decision Tree Clasification\n",
    "\n",
    "Decision trees are non-parametric supervised learning methods used for classification. The main aim of this is to define a model that gives value of target feature by learning decision rule inferred from data features.<br><br>\n",
    "Fitting the decision tree classifier with default values and `random state = 786` which was selected at the very beginning.<br><br>\n",
    "The score function returns the accuracy of classifier on the test data. Accuracy is ratio of total correctly <br>predicted observations upon total number of observations.The accuracy measured was 76.38%.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=786)\n",
    "dt_classifier.fit(D_train, t_train)\n",
    "dt_classifier.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Out of Criterion = `gini, entropy`, `min_sample_split = [2, 3, 4]` & `max_depth = [1, 2, 3, 4, 5, 6, 7, 8]` the grid search function finds out the best parameter values and calculates the model score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model, dt_best_estimate, dt_result = grid_search(D_train, t_train, dt_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*With Criterion `gini` , `max_depth = 4` and `min_sample_splits` of 2 the model predicts the accuracy 76.6%*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_DT = pd.DataFrame(dt_result.cv_results_['params'])\n",
    "results_DT['test_score'] = dt_result.cv_results_['mean_test_score']\n",
    "results_DT.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the DT Performance Comparison \n",
    "Also from the plot we visualise the best hyperparamters as `gini` and `max_depth:4`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(results_DT, \n",
    "          title='DT Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('max_depth', title='Maximum Depth'),\n",
    "    alt.Y('test_score', title='Mean CV Score', aggregate='average', scale=alt.Scale(zero=False)),\n",
    "    color='criterion'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Advantages of Decision tree classifier\n",
    "*\tInexpensive to construct. \n",
    "*\tEasy to interpret for small size trees. \n",
    "*\tFast at classifying unknown records.\n",
    "\n",
    "#### Disadvantages\n",
    "*\tDecision tree models are often biased towards splits on features.\n",
    "*\tLarge trees can be difficult to interpret.\n",
    "*\tSmall change in training data can account for large change to decision logic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Random Forest classifier\n",
    "A random forest is a Meta estimator that fits number of decision tree classifier on various sub-samples and uses mean to advance the accuracy and avoid over-fitting.<br><br>\n",
    "Fitting the random forest classifier with default estimator `n = 100` i.e. number of trees in the forest,<br> criterion `gini` and `max_depth` 2.<br>\n",
    "\n",
    "The score function returns the accuracy of classifier on the test data. Accuracy is ratio of total correctly predicted observations upon total number of observations. The accuracy measured was *73.6%*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=786,n_estimators=100,max_depth=2,criterion='gini')\n",
    "rf_classifier.fit(D_train, t_train)\n",
    "rf_classifier.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Out of the given parameters given to grid search function `criterion = [‘gini’, ‘entropy’]`, `n_estimators = [110, 130, 150, 200]`, `max_depth = [3, 4, 5]` and `min_sample_split = [2, 3, 4]` it calculates & returns best parameters with model score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model, rf_best_estimate, rf_result = grid_search(D_train, t_train, rf_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The model predicts the accuracy score of 76.9%.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_RF = pd.DataFrame(rf_result.cv_results_['params'])\n",
    "results_RF['test_score'] = rf_result.cv_results_['mean_test_score']\n",
    "results_RF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the RF Performance Comparison \n",
    "From the plot we visualise that at `max_depth = 4` , `gini` overpowers `entropy`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(results_RF, \n",
    "          title='RF Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('max_depth', title='Maximum Depth'),\n",
    "    alt.Y('test_score', title='Mean CV Score', aggregate='average', scale=alt.Scale(zero=False)),\n",
    "    color='criterion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Random forest classifier\n",
    "*\tNo need of any feature selection \n",
    "*\tEasier to make parallel models \n",
    "*\tIf larger parts of features are lost , accuracy can still be maintained.\n",
    "\n",
    "#### Disadvantages\n",
    "*\tFits for some noisy data \n",
    "*\tTime complexity- much harder and time consuming to construct.\n",
    "\n",
    "#### Limitations\n",
    "*\tHeavy computation resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Support Vector Machine classifier \n",
    "SVM is linear model for classification problem. The idea of SVM is simple. The algorithm creates <br>a line or hyperplane which separates the data into classes.<br><br>\n",
    "We fit the model with default kernel as rbf and regularisation value=1.0 parameters .<br><br>\n",
    "The score function returns the accuracy of classifier on the test data. Accuracy is ratio of total<br> correctly predicted observations upon total number of observations. The accuracy measured was 78.6%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(D_train, t_train)\n",
    "svm_classifier.score(D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The parameters passed to grid search function were gamma `values=[0.1, 0.05, 0.001, 1]` as the value <br> must be between 0.1 to 1. `Kernels=[‘rbf’, ‘poly’, ‘sigmoid’]` with `C=[1, 10, 50, 100]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model, svm_best_estimate, svm_result = grid_search(D_train, t_train, svm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_best_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.score(D_train, t_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The model predicts the accuracy score of 83.4% with best parameters .*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_SVM = pd.DataFrame(svm_result.cv_results_['params'])\n",
    "results_SVM['test_score'] = svm_result.cv_results_['mean_test_score']\n",
    "results_SVM.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the SVM Performance Comparison \n",
    "From the plot we visualise that at max_depth =4 , gini overpowers entropy . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(results_SVM, \n",
    "          title='SVM Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('C', title='Regularisation Parameter'),\n",
    "    alt.Y('test_score', title='Mean CV Score', aggregate='average', scale=alt.Scale(zero=False)),\n",
    "    color='kernel'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of Support Vector Machine\n",
    "*\tWork well when there is clean margin of separation.\n",
    "*\tMemory efficient\n",
    "#### Disadvantages\n",
    "*\tNot suitable for larger data sets\n",
    "*\tSVM does not perform well when data set has more noise or target class is overlapping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison\n",
    "After testing the classifier  by considering the train data  and using it in cross validation way, <br>we know perform the paired t-test in order to understand if the difference between performance is statistically<br> significant for any 2 classifiers.<br><br>\n",
    "Firstly we calculate the cross_val_score  and then compare it with all models as:\n",
    "*\tKNN-DT\n",
    "*\tKNN-RF\n",
    "*\tKNN-SVM\n",
    "*\tDT-RF\n",
    "*\tDT-SVM\n",
    "*\tRF_SVM<br>\n",
    "\n",
    "From scipy library we import the stats module to run the t-test .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "cv_method_ttest = StratifiedKFold(n_splits=10, random_state=786)\n",
    "cv_results_KNN = cross_val_score(estimator=knn_model, X=Data, y=target, cv=cv_method_ttest, n_jobs=-1, scoring='accuracy')              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_KNN.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_RF = cross_val_score(estimator=rf_model, X=Data, y=target, cv=cv_method_ttest, n_jobs=-1, scoring='accuracy')\n",
    "cv_results_RF.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_DT = cross_val_score(estimator=dt_model, X=Data, y=target, cv=cv_method_ttest, n_jobs=-1, scoring='accuracy')\n",
    "cv_results_DT.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results_SVM = cross_val_score(estimator=svm_model, X=Data, y=target, cv=cv_method_ttest, n_jobs=-1, scoring='accuracy')\n",
    "cv_results_SVM.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(stats.ttest_rel(cv_results_KNN, cv_results_DT))\n",
    "print(stats.ttest_rel(cv_results_KNN, cv_results_RF))\n",
    "print(stats.ttest_rel(cv_results_KNN, cv_results_SVM))\n",
    "\n",
    "print(stats.ttest_rel(cv_results_DT, cv_results_RF))\n",
    "print(stats.ttest_rel(cv_results_DT, cv_results_SVM))\n",
    "\n",
    "print(stats.ttest_rel(cv_results_RF, cv_results_SVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The Pair KNN-SVM gives statistically significant value of 0.0002 which is less than 0.05.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation \n",
    "Model evaluation is one of the important step required to determine the best model, how well the model will perform.<br><br>\n",
    "The target variable for our dataset was multinomial. That is target feature is categorical with 4 different <br>level {0, 1, 2, 3}. It refers to different price range ={‘low’, ‘mid’, ‘high’, ‘v high’}. <br>Hence we cannot use binary metric such as roc_auc curve to evaluate multinomial classifier.<br><br>\n",
    "Below are the evaluation metrics used to find the accuracy, classification report and average model accuracy<br> for each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def print_model_stats(model, D_test, t_test):\n",
    "    pred = model.predict(D_test)\n",
    "    print(\"=========={model_name} Model Statistics=============\".format(model_name=model.__class__.__name__))\n",
    "    print(\"Accuracy score:\", metrics.accuracy_score(t_test, pred))\n",
    "    print(\"Confusion Matrix:\\n\", metrics.confusion_matrix(t_test, pred))\n",
    "    print(\"Classification report:\\n\", metrics.classification_report(t_test, pred))\n",
    "    print(\"Average model accuracy:\", metrics.balanced_accuracy_score(t_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(knn_model, D_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(dt_model, D_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(rf_model, D_test, t_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_model_stats(svm_model, D_test, t_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hence we conclude that SVM gives us the best model accuracy and should be used for this predicting target feature.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Conclusion\n",
    "After cleaning and visualizing the dataset, we were able to find clear pattern for features like `ram` which was proportional to the `price_range`, whereas some features had a very little information gain like `bluetooth` and `wifi`. We also noticed a pattern in `screen-size` for phones, had 2 distinct sizes which were manufactured the most, which aligned with the popular screen sizes provided by big brands(like Apple's iPhone).\n",
    "\n",
    "The case study was to predict the cell phone price based on the descriptive features. We have successfully built a model based on the parameters given by grid search. That is we have fine-tuned the parameters and the best ones were applied to the model to train the data. The model was then tested and accuracy was computed for each algorithms. Out of 4, SVM gave us best accuracy with 84%.<br><br> \n",
    "\n",
    "Also we performed statistically significant ttest to determine if any difference between performance of any two classifier and we got KNN-SVM results as significant. Last but not the least, we used method evaluation techniques to verify the accuracy for multinomial classifier and it gave the same results.\n",
    "\n",
    "There were also some limitations. The f-score method does not reveal information among the features but still we have used due to greater score than random forest importance.\n",
    "\n",
    "Also we used few cases for feature selection and parameter tuning .we could have explored more taken more parameters and more feature selection methods. This might had helped us giving a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "- Sharma, A. (2018). Mobile Price Classification [online]. Retrieved from https://www.kaggle.com/iabhishekofficial/mobile-price-classification\n",
    "\n",
    "- Vural, A. (2019). Feature Ranking [online]. Retrieved from http://www.featureranking.com\n",
    "\n",
    "- Pupale, R. (2018). Support Vector Machines(SVM) — An Overview [online]. Retrieved from https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('venv': venv)",
   "language": "python",
   "name": "python37764bitvenvvenva7ca7c3fbdff451194a7ae3e78f6fd2f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}